{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 004_autoGrad.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bensayah/Applied-Deep-Learning/blob/main/Copy_autoGrad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rte1f13L-qcW"
      },
      "source": [
        "![datahacker.png](data:image/png;base64,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)\n",
        "# Computation graphs and Autograd in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV_NWJXPS3O_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPONq8bfte96"
      },
      "source": [
        "!pip install autograd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADijgaz8twXM"
      },
      "source": [
        "import autograd.numpy as np\n",
        "from autograd import grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxKV_erzEdk7"
      },
      "source": [
        "# a named Python function\n",
        "def g(w):\n",
        "    return np.tanh(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPuIeLs4Ec5-"
      },
      "source": [
        "# how to use the 'tanh' function\n",
        "w_val = 1.0   # a test input for our 'sin' function\n",
        "g_val = g(w_val)\n",
        "print (g_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcOSl8ruEb0p"
      },
      "source": [
        "# how to use 'lambda' to create an \"anonymous\" function - just a pithier way of writing functions in Python\n",
        "g = lambda w: np.tanh(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hJrxKBoE8Qm"
      },
      "source": [
        "# how to use the 'sin' function written in 'anonymous' format\n",
        "w_val = 1.0   \n",
        "g_val = g(w_val)\n",
        "print (g_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv3p3vKFE7Km"
      },
      "source": [
        "# if we input a float value into our function, it returns a float\n",
        "print (type(w_val))\n",
        "print (type(g_val))\n",
        "\n",
        "# if we input a numpy array, it returns a numpy array\n",
        "w_val = np.array([1.0])  \n",
        "g_val = g(w_val)\n",
        "print (g_val)\n",
        "print (type(w_val))\n",
        "print (type(g_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHAl362aE6R1"
      },
      "source": [
        "# import statment for gradient calculator\n",
        "from autograd import grad  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeGf58WnFfuC"
      },
      "source": [
        "# create the derivative/gradient function of g --> called dgdw\n",
        "dgdw = grad(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHwvGh6CFeVY"
      },
      "source": [
        "# evaluate the gradient function at a point\n",
        "w_val = 1.0\n",
        "print(dgdw(1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYYpfItHK59S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOneqI2uFdKw"
      },
      "source": [
        "# compute the second derivative of our input function\n",
        "dgdw2 = grad(dgdw)\n",
        "print (dgdw(1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o4HzgBQE43D"
      },
      "source": [
        "# import statement for gradient calculator - this method will return both\n",
        "# derivative and function evaluations (the latter always being computed 'under\n",
        "# the hood')\n",
        "from autograd import value_and_grad \n",
        "\n",
        "# how to use 'lambda' to create an \"anonymous\" function - just a pithier way of writing functions in Python\n",
        "g = lambda w: np.tanh(w)\n",
        "\n",
        "# create the derivative/gradient function of g --> called dgdw\n",
        "dgdw = value_and_grad(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9A30Qp0GSio"
      },
      "source": [
        "# evaluate the gradient function at a point\n",
        "w_val = 0.0\n",
        "print (dgdw(0.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjpaMfafGQjQ"
      },
      "source": [
        "# create function / first derivative\n",
        "g = lambda w: np.tanh(w) \n",
        "dgdw = grad(g)\n",
        "\n",
        "# create first order taylor series approximation - as a Python function\n",
        "# here w_0 is the center point of the expansion\n",
        "first_order = lambda w_0,w: g(w_0) + dgdw(w_0)*(w - w_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMzi-loXGOlP"
      },
      "source": [
        "# create second derivative function\n",
        "dgdw2 = grad(dgdw)\n",
        "    \n",
        "# create second order taylor series approximation - as a Python function\n",
        "# here w_0 is the center point of the expansion\n",
        "second_order = lambda w_0,w: g(w_0) + dgdw(w_0)*(w - w_0) + 0.5*dgdw2(w_0)*(w - w_0)**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KTHeOvuGNVy"
      },
      "source": [
        "# a simple multi-input function defined in python \n",
        "def g(w_1,w_2):\n",
        "    return np.tanh(w_1*w_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqFvPKMdGM9j"
      },
      "source": [
        "# compute the gradient of our input function\n",
        "dgdw1 = grad(g,0)\n",
        "dgdw2 = grad(g,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVZyvbqXLbeC"
      },
      "source": [
        "print(dgdw1(1.0,-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0JLr0ClL74a"
      },
      "source": [
        "print(dgdw2(1.0,-2.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaRtxHBLICqZ"
      },
      "source": [
        "# construct all partial derivative functions at once\n",
        "nabla_g = grad(g,(0,1))\n",
        "print(nabla_g(1.0,-2.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujrPzLDBIBTx"
      },
      "source": [
        "# a simple multi-input function defined in python \n",
        "def g(w_1,w_2):\n",
        "    return np.tanh(w_1*w_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXOn1oe4IAQC"
      },
      "source": [
        "# import function flattening module from autograd\n",
        "from autograd.misc.flatten import flatten_func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQfTuJxkJO5d"
      },
      "source": [
        "# flatten an input function g\n",
        "g, unflatten_func, w = flatten_func(f, input_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ59xt-bJNXH"
      },
      "source": [
        "# Python implementation of the function above\n",
        "z = np.ones((N,1))\n",
        "def f(input_weights):\n",
        "    a = input_weights[0]\n",
        "    b = input_weights[1]\n",
        "    C = input_weights[2]\n",
        "    return (((a + np.dot(z.T,b) + np.dot(np.dot(z.T,C),z)))**2)[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMRT3ARxJL_F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaYWFF-uJLBu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMCmcshRJJ-b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXRay8yFuI9J"
      },
      "source": [
        "# Define a function like normal with Python and Numpy\n",
        "def tanh(x):\n",
        "   y = np.exp(-x)\n",
        "   return (1.0 - y) / (1.0 + y)\n",
        "# Create a function to compute the gradient\n",
        "grad_tanh = grad(tanh)\n",
        "# Evaluate the gradient at x = 1.0# how to use 'lambda' to create an \"anonymous\" function - just a pithier way of writing functions in Python\n",
        "g = lambda w: np.tanh(w)\n",
        "print(grad_tanh(1.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0k0gY0tfubUC"
      },
      "source": [
        "# Define a function like normal with Python and Numpy\n",
        "def h(x,y):\n",
        "    z = -2*x**2 + y\n",
        "    return z\n",
        "# Create a function to compute the gradient\n",
        "grad_h = grad(h,(0,1))\n",
        "grad_h0 = grad(h,0)\n",
        "grad_h1 = grad(h,1)\n",
        "# Evaluate the gradient at x = 0.0\n",
        "t=[0.,1.]\n",
        "print(grad_h0(2.,0.))\n",
        "print(grad_h0(t[0],t[1]))\n",
        "print(grad_h1(0.,1.))\n",
        "print(grad_h(0.,1.))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iX__NJIIF1a"
      },
      "source": [
        "## Simple example with gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw-BWzT_BlsQ"
      },
      "source": [
        "Let's now see a simple example of how the derivative is calculated. We will create a scalar tensor `x` and set the `requires_grad` parameter to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOt6cjXjBmDb"
      },
      "source": [
        "x = torch.tensor([3.,3.], requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwk1w6AoVSk8"
      },
      "source": [
        "We will calculate `y` the following way:\\\n",
        "$ y = 3x^2 + 4x + 2$\n",
        "\n",
        "Now let's see what we get when we replace the `x` with our value `3`:  \n",
        "$ y = 3(3)^2 + 4(3) + 2 $\\\n",
        "$ y = 3*9 + 12 + 2$\\\n",
        "$ y = 27 + 12 + 2 $\\\n",
        "$ y = 41 $\n",
        "\\\n",
        "Now comes the part were we take the derivative of `y` with respect to the variable `x`.\\\n",
        "$\\frac{dy}{dx} = 2*3x + 4 = 6x + 4$\\\n",
        "If we replace `x` with our value `3`, we get the following:\n",
        "$6x + 4 = 6(3) + 4 = 18 + 4 = 22$\\\n",
        "So the gradient is equal to $22$\\\n",
        "Let's see how we can do this in code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DRxYu1cB7Zc"
      },
      "source": [
        "y = 3*x**2 + 4*x + 2\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH73ZIrsMVGR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDV33SPOvR14"
      },
      "source": [
        "y.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ABNPzN5MohN"
      },
      "source": [
        "#x.grad.zero_()\n",
        "z = (y**2).mean()\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHW8G69mO2Q_"
      },
      "source": [
        "z.backward()\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICDKhG8CPebJ"
      },
      "source": [
        "41**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr97RVIfXHF5"
      },
      "source": [
        "Call `y.backward()` to calculate the derivative for that function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGA2TTZ2a72l"
      },
      "source": [
        "## Is there a way to turn off the gradient calculation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3myJit2O93s"
      },
      "source": [
        "The answer is yes, you can turn the gradient calculation anytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlL9kf-UbEaQ"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcO3N1fMVYS0"
      },
      "source": [
        "x = x.requires_grad_(False)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSQmsP8mVgfc"
      },
      "source": [
        "x = x.detach()\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFiwtMAiWX9_"
      },
      "source": [
        "# Gradient accumulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUcL2csoPiov"
      },
      "source": [
        "The auto gradient calculation does not reset the gradients automatically, therefore we have to reset them after each optimization. If we forget this step they could end up just accumulating.\\\n",
        "This sounds complex, but it is not, it's easy. To reset the gradients for a particular tensor, you can simply pass `x.grad.zero_()` and it will reset the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxLVo9P_WS2G"
      },
      "source": [
        "x = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  y = 3*x**2 + 4*x + 2\n",
        "  y.backward()\n",
        "\n",
        "  print(x.grad)\n",
        "  x.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEO9m6PElhrw"
      },
      "source": [
        "## Optimizing parameters with autograd "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33aJwXNpQ6QJ"
      },
      "source": [
        "Create `x` and `y` data and transform them to be torch tensors using the function `torch.from_numpy()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZV2JHP1KIQ"
      },
      "source": [
        "x = torch.tensor([1.,2.,3.], requires_grad=True)\n",
        "y = x * 2 + 3\n",
        "z = y ** 2\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIGO5Md-4IED"
      },
      "source": [
        "out = z.mean()\n",
        "out.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUfMuTKC449H"
      },
      "source": [
        "print(out)\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED_QFLX_Ar5m"
      },
      "source": [
        "v = torch.tensor([1.,1.,1.])\n",
        "z.backward(v)\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75WH8TaSJ-s6"
      },
      "source": [
        "v = torch.tensor([1.,1.,1.])\n",
        "z.backward(v)\n",
        "print(x.grad/len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-jlM9DcRE8N"
      },
      "source": [
        "Create a list for `w` and `b` in which we will save those values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGSWjSf4NIm8"
      },
      "source": [
        "x = torch.tensor([1.,2.,3.,4.,5])\n",
        "y = x * 2 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY4ovV3UNo3S"
      },
      "source": [
        "w_ = torch.tensor(5., requires_grad=True)\n",
        "b_ = torch.tensor(1., requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Wa9-7MNo6y"
      },
      "source": [
        "for i_value in range(len(x)):\n",
        "  w = torch.tensor(5., requires_grad=True)\n",
        "  b = torch.tensor(1., requires_grad=True)\n",
        "\n",
        "  y_hat = w * x[i_value] + b\n",
        "  \n",
        "  error = y_hat - y[i_value]\n",
        "  loss = error ** 2 \n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  alpha = 0.01\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    w_.data -= alpha * (w.grad/len(x))\n",
        "    b_.data -= alpha * (b.grad/len(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0crGHfd5RNNW"
      },
      "source": [
        "We will iterate and calculate the gradients for each value from the tensor `x_torch`. If we want to calculate the gradient for each number instead of the whole batch or list of numbers, we need to set the `w` and `b` parameters to the same number after calculation. After this step we do the forward pass for that number, and then we calculate the loss for that number and do the backpropagation. Update the `w` and `b` parameters and save them in the list we initialized earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSghVw7USY2q"
      },
      "source": [
        "To get the final gradient we need to sum all of the gradients for all the numbers and divide them with the total number of values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9TIlFQfe47K"
      },
      "source": [
        "print(w_)\n",
        "print(b_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEM3E6LMyQey"
      },
      "source": [
        "w_ = torch.tensor(5., requires_grad=True)\n",
        "b_ = torch.tensor(1., requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7BuAfcFyW1N"
      },
      "source": [
        "y_hat = w * x + b\n",
        "error = y_hat - y\n",
        "loss = error ** 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcPYM5nNyhFw"
      },
      "source": [
        "loss=loss.mean()\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQCBKwfCysQX"
      },
      "source": [
        "w.data -= alpha * w.grad\n",
        "b.data -= alpha * b.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRi5BA-7SfyA"
      },
      "source": [
        "Let's now see how we can do the same thing but just with the whole set of numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1JgTpKleZec"
      },
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}